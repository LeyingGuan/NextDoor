<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">



<title>Nextdoor Analysis Vignette</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Nextdoor Analysis Vignette</h1>



<div id="leying-guan-and-rob-tibshirani" class="section level3">
<h3>Leying Guan and Rob Tibshirani</h3>
<div id="yale-october-1-2019" class="section level4">
<h4>Yale October 1, 2019</h4>
<p>Introduction and installation</p>
<blockquote>
<p><a href="#intro">Introduction</a></p>
</blockquote>
<blockquote>
<p><a href="#install">Installation</a></p>
</blockquote>
<p>Nextdoor analysis for generalized liner regression with lasso penalty</p>
<blockquote>
<p><a href="#glmnet">nextdoor.glmnet</a></p>
</blockquote>
<p>Nextdoor analysis for general supervised learning algorithms</p>
<blockquote>
<p><a href="#supervised1">nextdoor</a></p>
</blockquote>
<blockquote>
<p><a href="#supervised2">getIndex</a></p>
</blockquote>
<p><a id="intro"></a></p>
</div>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Nextdoor is a package that performs feature indispensability test after model selection. For any feature <span class="math inline">\(j\)</span> and any pre-fixed training procedure <span class="math inline">\(\mathcal{M}\)</span>, it access the indispensability of feature <span class="math inline">\(j\)</span> by considering whether excluding <span class="math inline">\(j\)</span> will lead to deterioration in the out-of-sample prediction power if we use the training procedure <span class="math inline">\(\mathcal{M}\)</span>. Formally speaking, we are interested in the following hypothesis testing problem:</p>
<p><span class="math display">\[
H_0: Err(j, \mathcal{M}) \leq Err(\mathcal{M}) \quad vs. \quad H_1: Err(j,\mathcal{M}) &gt; Err(\mathcal{M}).
\]</span></p>
<p>Here Err(j,<span class="math inline">\(\mathcal{M}\)</span>) is the out-of-sample test error if we apply the training procedure <span class="math inline">\(\mathcal{M}\)</span> to our data set without feature <span class="math inline">\(j\)</span> and Err(<span class="math inline">\(\mathcal{M}\)</span>) is the out-of-sample test error if we apply the training procedure <span class="math inline">\(\mathcal{M}\)</span> to our data set with all features.</p>
<p>As an example, we consider a linear regression problem with lasso penalty.</p>
<p><span class="math display">\[
\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} (y_i-\beta_0-\beta^T x_i)^2 + \lambda ||\beta||_1,
\]</span> The training procedure <span class="math inline">\(\mathcal{M}\)</span> is to apply the model above to our data set. For any feature <span class="math inline">\(j\)</span>, the same procedure without it is to consider the model <span class="math display">\[
\min_{\beta_0,\beta:\beta_j = 0} \frac{1}{N} \sum_{i=1}^{N} (y_i-\beta_0-\beta^T x_i)^2 + \lambda ||\beta||_1,
\]</span> We can use cross-validation to as an proxy to the out-of-sample test errors.</p>
<p>In practice, both the penalty value <span class="math inline">\(\lambda\)</span> and the feature <span class="math inline">\(j\)</span> to look at are data adaptive. For examples, we may pick the <span class="math inline">\(\lambda\)</span> that achieves the smallest cross-validation error, and we may choose to look at the feature <span class="math inline">\(j\)</span> that is in the original selected model.</p>
<p>Next-door analysis taken into account the selection bias and randomness in the above selections by</p>
<pre><code> 1. Debiasing the selected cross-validation error estimates.
 2. Taking into consideration  the probability of any feature being selected.</code></pre>
<p>By the end, the Next-door analysis tries to answer a non-random question of whether we can reject <span class="math inline">\(H_0\)</span> or not. We find that it has good empirical performance compared with naive method that neglects the selections. Detailed description of the Nextdoor analysis can be found in [1].</p>
<p>[1]Guan, Leying, and Robert Tibshirani. “Post model-fitting exploration via a” Next-Door&quot; analysis.&quot; arXiv preprint arXiv:1806.01326 (2018).</p>
<p><a id="install"></a></p>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>Like many other R packages, the simplest way to obtain <code>glmnet</code> is to install it directly from CRAN. Type the following command in R console:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;nextdoor&quot;</span>, <span class="dt">repos =</span> <span class="st">&quot;https://cran.us.r-project.org&quot;</span>)</a></code></pre></div>
<p>Users may change the <code>repos</code> options depending on their locations and preferences. Other options such as the directories where to install the packages can be altered in the command. For more details, see <code>help(install.packages)</code>.</p>
<p>Here the R package has been downloaded and installed to the default directories.</p>
<p>The user can also install the package using source files in github by the following commands:</p>
<p>library(devtools)</p>
<p>install_github(“LeyingGuan/NextDoor/nextdoor”)</p>
<p><a id="glmnet"></a></p>
</div>
<div id="nextdoor.glmnet" class="section level2">
<h2>nextdoor.glmnet</h2>
<p>We cam apply the Next-door analysis to post-selection lasso regression model corresponds to the model minimizing the cross-validation error or applying the one-standard deviation rule using cv.glmnet. The function proximity.glmnet performs model training, model selection, unbiased error estimation and the test (p-value/model score) for cv.glmnet.</p>
<p>nextdoor.glmnet {nextdoor} R Documentation Perform training, model selection, unbiased error estimation and the proximity test(p-value/model score) for functions in glmnet Description Perform model selection, unbiased error estimation and the nextdoor test(p-value/model score) for functions in glmnet.</p>
<p>The two most important quantities nextdoor.glmnet produces are (1) p-value, which is the Bootstrap p-value using the debiased test error estimation without considering only the model selection, (2) model score, which is calculated as</p>
<p><span class="math display">\[model\; score = \frac{p\;value}{selection\;frequency}\]</span></p>
<p>for every feature we are interested in.</p>
<div id="usage" class="section level3">
<h3>Usage</h3>
<p>nextdoor.glmnet(x, y, cv_glm, nams = NULL, family = “gaussian”, lossfun = NULL, standardize = T, K = 100, B = 1000, alpha = 0.1, epsilon = 0.05^2, epsilon2 =0.05^2, selectionType = 0, Bindex = NULL, pv = TRUE, rescale = TRUE, score = TRUE, B1 = 50, Bindex1 = NULL,trace = TRUE)</p>
<div id="arguments" class="section level4">
<h4>Arguments</h4>
<ul>
<li><p><code>x</code>: n by p training feature matrix</p></li>
<li><p><code>y</code>: Length p response vector</p></li>
<li><p><code>nams</code>: a length p vector containing feature names. By default, nams = NULL, the features are named according to their column indexes in x.</p></li>
<li><p><code>cv_glm</code>: Return from the function cv.glmnet keep = T.</p></li>
<li><p><code>family</code>: Response type, it can be one of “gaussian”,“binomial”,“poisson”,“multinomial”. By default, family = “gaussian”. It should be consistent with cv_glm.</p></li>
<li><p>lossfun A user-specific loss function for model evaluation. If loss = NULL, by default, we will use the deviance.</p></li>
<li><code>standardize</code>: Whether to standardize the data, standardize = T by default. It should be consistent with cv_glm.</li>
<li><code>K</code>: number of repetitions estimating the de-biased error.</li>
<li><p><code>B</code>: number of bootstrap repetitions estimating the p-value.</p></li>
<li><p>alpha: added errors’ level with added errors being covariance structure*alpha. By default, alpha = .1.</p></li>
<li><p><code>epsilon</code>: added errors’ level with added errors being identity*min(covariance diagonal)*epsilon. By default, epsilon = 0.05^2.</p></li>
<li><p>`epsilon2’: added errors’ level in the Bootstrap with added errors being min(covariance diagonal)*epsilon2. By default, epsilon2 = 0.05^2</p></li>
</ul>
<p>*<code>Bindex</code>: n by B index matrix for bootstrap. If Bindex == NULL, use the randomly generated bootstrap samples, otherwise, use the provide matrix.</p>
<p>*<code>selectionType</code>: if selectionType == 0, pick the model with the smallest randomized error, if selectionType == 1, use the 1se rule.</p>
<ul>
<li><code>pv</code>: if pv == True, estimate the p-values.</li>
</ul>
<p>*<code>rescale</code>: if rescale == True, perform the mean-rescaled Bootstrap.</p>
<ul>
<li><p><code>score</code>: if score == True, provide model scores.</p></li>
<li><p><code>B1</code>: number of repetitions for the paired Bootstrap used to create model score.</p></li>
<li><p><code>Bindex1</code>: n by B1 index matrix for paired bootstrap in the model score step. If Bindex1 == NULL, use the default Bootstrap, otherwise, use the provide matrix.</p></li>
</ul>
<p>*<code>trace</code>: if trace == True, print the p-values as they are calculated.</p>
</div>
<div id="value" class="section level4">
<h4>Value</h4>
<ul>
<li><p><code>model0</code>: original model sequence</p></li>
<li><p><code>models</code>: list of proximal model sequences</p></li>
<li><p><code>errors0</code>: original error matrix</p></li>
<li><p><code>errors</code>: list of proximal model matrices</p></li>
<li><p><code>debiased_errors0</code>: de-biased estimate of the prediction error for the original process</p></li>
<li><p><code>debiased_errors</code>: de-biased estimates of the prediction error for the processes excluding a each of the selected feature</p></li>
<li><p><code>worsen</code>: estimated increase in prediction error</p></li>
<li><p><code>p_value</code>: p values for proximity analysis</p></li>
<li><p><code>selection_frequency</code>: frequency of features in S being selected</p></li>
<li><p><code>model_score</code>: model_score for proximity analysis</p></li>
<li><p><code>result_table</code>: organized result table</p></li>
</ul>
</div>
<div id="examples" class="section level4">
<h4>Examples</h4>
<p>data(prostateCancerData)</p>
<p>data_train = prostateCancerData$train</p>
<p>data_test = prostateCancerData$test</p>
<p>x = data_train$feature</p>
<p>y = data_train$response</p>
<p>nams = data_train$names</p>
<p>n=length(y)</p>
<p>set.seed(483)</p>
<p>R1 = proximity.glmnet(X = x, Y = y, family = “gaussian”, nfolds = 10, nlambda = 30, standardize = F, alpha = .1,epsilon = 0.05^2, epsilon2 = .05^2, B = 1000, B1 = 20)</p>
<p>print(round(R1$result_table,3))</p>
<p><a id="supervised1"></a></p>
</div>
</div>
</div>
<div id="getindex" class="section level2">
<h2>getIndex</h2>
<p>The function getIndex chooses a model based on randomized error curve. The users can use it to pick a model for any supervised learning algorithms they want to implement.</p>
<div id="usage-1" class="section level3">
<h3>Usage</h3>
<p>getIndex(errors0, alpha = 0.1, epsilon = 0.1, selectionType = 0, one_sds = rep(0, ncol(errors0)))</p>
</div>
<div id="arguments-1" class="section level3">
<h3>Arguments</h3>
<ul>
<li><p><code>errors0</code>: the n by m errors of the original model sequence</p></li>
<li><p><code>alpha</code>: added error with covariance structure*alpha, by default, alpha = .1</p></li>
<li><p><code>epsilon</code>: added error with covariance structure being identity times min(covariance diagonal) times epsilon, by default, epsilon = 0.1</p></li>
<li><p><code>selectionType</code>: if selectionType == 0, pick the model with the smallest randomized error</p></li>
<li><p><code>one_sds</code>: if the selectionType is 1, the we choose the model with smallest index such that model error(randomized) &lt;= one_sds[i] + min error (randomized).</p></li>
</ul>
</div>
<div id="value-1" class="section level3">
<h3>Value</h3>
<ul>
<li><code>model_index</code>: selected model index in the original model sequences based on the randomized error curve.</li>
</ul>
<p><a id="supervised2"></a></p>
</div>
</div>
<div id="nextdoor" class="section level2">
<h2>nextdoor</h2>
<p>We can also apply the Nextdoor analysis to other supervised algorithms using the function proximity when the original model is picked using the function getIndex. It requires the user to have the prediction errors for the original model sequence and the errors for the next-door model sequences available.</p>
<div id="usage-2" class="section level3">
<h3>Usage</h3>
<p>nextdoor(errors0, errors, S, nams=NULL, K = 100, B = 1000, alpha = 0.1, epsilon = 0.05^2, epsilon2 = 0.05^2,Bindex = NULL,pv = TRUE, rescale = TRUE, selectionType = 0, one_sds = rep(0, ncol(errors0)),trace = T) ### Arguments</p>
<ul>
<li><p><code>errors0</code>: the n by m errors of the original model sequence</p></li>
<li><p><code>errors</code>: a list of n by m errors of the proximal model sequences</p></li>
<li><p><code>S</code>: a vector corresponding of names of the proximal sequences (name in errors)</p></li>
<li><p><code>nams</code>: a length p vector containing feature names. By default, nams = NULL, the features are named according to their column indexes in x.</p></li>
<li><p><code>K</code>: number of repetitions estimating the de-biased error</p></li>
<li><p><code>B</code>: number of bootstrap repetitions</p></li>
<li><p><code>alpha</code>: added error with covariance structure*alpha, by default, alpha = .1</p></li>
<li><p><code>epsilon</code>: added error with covariance structure being indentity times min(covariance diagonal) times epsilon, by default, epsilon = 0.05^2.</p></li>
<li><p><code>epsilon2</code>: added error in the Bootstrap step being min(covariance diagonal) times epsilon2, by default, epsilon2 = 0.05^2</p></li>
<li><p><code>Bindex</code>: n by B index matrix for bootstrap. if Bindex == NULL, use the default Bootstrap, otherwise, use the provide matrix.</p></li>
<li><p><code>pv</code>: if pv == True, estimate the p-values</p></li>
<li><p><code>rescale</code>: if rescale == True, perform the mean-rescaled Bootstrap</p></li>
<li><p><code>selectionType</code>: if selectionType == 0, pick the model with the smallest randomized error if selectionType == 1, use the 1se rule</p></li>
<li><p><code>one_sds</code>: if the selectionType is 1, the we choose the model with smallest index such that model error(randomized) &lt;= one_sds[i] + min error(randomized)</p></li>
<li><p><code>trace</code>: if trace == True, print the p-value process</p></li>
</ul>
</div>
<div id="value-2" class="section level3">
<h3>Value</h3>
<ul>
<li><p><code>debiased_errors0</code>: de-biased estimate of the model error for the original procedure</p></li>
<li><p><code>debiased_errors</code>: de-biased estimate of the model error for the process excluding a specific feature</p></li>
<li><p><code>worsen</code>: estimated increase in prediction error</p></li>
<li><p><code>pv_value</code>: p values for proximity analysis</p></li>
<li><p><code>result_table</code>: organized result table ### Example set.seed(48) alphas = c(1:10)/10; errors0 = array(NA, dim = c(length(data<span class="math inline">\(response),length(alphas))); errors=list() for(i in 1:length(alphas)){  model0 = ranger(response~., data = data, alpha = alphas[i])  errors0[,i] = (data\)</span>response-model0<span class="math inline">\(predictions)^2  for(j in 1:length(nams)){  if(is.null(errors[j][[1]])){errors[[j]] = array(0, dim = c(length(data\)</span>response),length(alphas)))} data1 = data[,-j];model0 = ranger(response~., data = data1, alpha = alphas[i]) errors[[j]][,i] = (data<span class="math inline">\(response-model0\)</span>predictions)^2 } } res = nextdoor(errors0 = errors0, errors = errors, S =c(1:length(nams)), nams=nams, B = 1000, alpha = 0.1, pv = TRUE,rescale = TRUE, selectionType = 0,trace = TRUE)</p></li>
</ul>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
