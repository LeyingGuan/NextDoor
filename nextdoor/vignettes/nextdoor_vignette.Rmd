---
title: "Nextdoor Analysis Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{nextdoor vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

### Leying Guan and Rob Tibshirani
#### Yale October 1, 2019
Introduction and installation

> [Introduction](#intro)

> [Installation](#install)

Nextdoor analysis for generalized liner regression with lasso penalty

> [nextdoor.glmnet](#glmnet)

Nextdoor analysis for general supervised learning algorithms

> [nextdoor](#supervised1)

> [getIndex](#supervised2)

<a id="intro"></a>

## Introduction
Nextdoor is a package that performs feature indispensability test after model selection. For any feature $j$ and any pre-fixed training procedure $\mathcal{M}$, it access the indispensability of feature $j$ by considering whether excluding $j$ will lead to deterioration in the out-of-sample prediction power if we use the training procedure $\mathcal{M}$. Formally speaking, we are interested in the following hypothesis testing problem:

$$
H_0: Err(j, \mathcal{M}) \leq Err(\mathcal{M}) \quad vs. \quad H_1: Err(j,\mathcal{M}) > Err(\mathcal{M}).
$$


Here Err(j,$\mathcal{M}$) is the out-of-sample test error if we apply the training procedure $\mathcal{M}$ to our data set without feature $j$ and Err($\mathcal{M}$) is the out-of-sample test error if we apply the training procedure $\mathcal{M}$ to our data set with all features.

As an example, we consider a linear regression problem with lasso penalty.

$$
\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} (y_i-\beta_0-\beta^T x_i)^2 + \lambda ||\beta||_1,
$$
The training procedure $\mathcal{M}$ is to apply the model above to our data set. For any feature $j$, the same procedure without it is to consider the model
$$
\min_{\beta_0,\beta:\beta_j = 0} \frac{1}{N} \sum_{i=1}^{N} (y_i-\beta_0-\beta^T x_i)^2 + \lambda ||\beta||_1,
$$
We can use cross-validation to as an proxy to the out-of-sample test errors.

In practice, both the penalty value $\lambda$ and the feature $j$ to look at are data adaptive. For examples, we may pick the $\lambda$ that achieves the smallest cross-validation error, and we may choose to look at the feature $j$ that is in the original selected model.

Next-door analysis taken into account the selection bias and randomness in the above selections by

     1. Debiasing the selected cross-validation error estimates.
     2. Taking into consideration  the probability of any feature being selected.
By the end, the Next-door analysis tries to answer a non-random question of whether we can reject $H_0$ or not. We find that it has good empirical performance compared with naive method that neglects the selections. Detailed description of the Nextdoor analysis can be found in [1].

[1]Guan, Leying, and Robert Tibshirani. "Post model-fitting exploration via a" Next-Door" analysis." arXiv preprint arXiv:1806.01326 (2018).
     

<a id="install"></a>

## Installation

Like many other R packages, the simplest way to obtain `glmnet` is to install it directly from CRAN. Type the following command in R console:

```{r, eval=FALSE}
install.packages("nextdoor", repos = "https://cran.us.r-project.org")
```

Users may change the `repos` options depending on their locations and preferences. Other options such as the directories where to install the packages can be altered in the command. For more details, see `help(install.packages)`.

Here the R package has been downloaded and installed to the default directories.

The user can also install the package using source files in github by the following commands:

library(devtools)

install_github("LeyingGuan/NextDoor/nextdoor")

<a id="glmnet"></a>

## nextdoor.glmnet

We cam apply the Next-door analysis to post-selection lasso regression model corresponds to the model minimizing the cross-validation error or applying the one-standard deviation rule using cv.glmnet. The function proximity.glmnet performs model training, model selection, unbiased error estimation and the test (p-value/model score) for cv.glmnet.



nextdoor.glmnet {nextdoor}	R Documentation
Perform training, model selection, unbiased error estimation and the proximity test(p-value/model score) for functions in glmnet
Description
Perform model selection, unbiased error estimation and the nextdoor test(p-value/model score) for functions in glmnet.

The two most important quantities nextdoor.glmnet produces are (1) p-value, which is the Bootstrap p-value using the debiased test error estimation without considering only the model selection, (2) model score, which is calculated as

$$model\; score = \frac{p\;value}{selection\;frequency}$$

for every feature we are interested in.

### Usage
nextdoor.glmnet(x, y, cv_glm, nams = NULL, family = "gaussian",  lossfun = NULL, standardize = T,
K = 100, B = 1000, alpha = 0.1, epsilon = 0.05^2, epsilon2 =0.05^2,
selectionType = 0, Bindex = NULL, pv = TRUE,  rescale = TRUE,
score = TRUE, B1 = 50, Bindex1 = NULL,trace = TRUE)

#### Arguments
* `x`: n by p training feature matrix

* `y`: Length p response vector

* `nams`: a length p vector containing feature names. By default, nams = NULL, the features are named according to their column indexes in x.

* `cv_glm`: Return from the function cv.glmnet keep = T.

* `family`: Response type, it can be one of "gaussian","binomial","poisson","multinomial". By default, family = "gaussian". It should be consistent with cv_glm.

* lossfun A user-specific loss function for model evaluation. If loss = NULL, by default, we will use the deviance.  

* `standardize`: Whether to standardize the data, standardize = T by default. It should be consistent with cv_glm.
* `K`: number of repetitions estimating the de-biased error.
* `B`: number of bootstrap repetitions estimating the p-value.

* alpha: added errors' level with added errors being covariance structure*alpha. By default, alpha = .1.

* `epsilon`: added errors' level  with added errors being identity\*min(covariance diagonal)\*epsilon. By default, epsilon = 0.05^2.

* `epsilon2': added errors' level in the Bootstrap with  added errors being min(covariance diagonal)*epsilon2. By default, epsilon2 = 0.05^2

*`Bindex`: n by B index matrix for bootstrap. If Bindex == NULL, use the randomly generated bootstrap samples, otherwise, use the provide matrix.

*`selectionType`: if selectionType == 0, pick the model with the smallest randomized error, if selectionType == 1, use the 1se rule.

* `pv`: if pv == True, estimate the p-values.

*`rescale`: if rescale == True, perform the mean-rescaled Bootstrap.

* `score`: if score == True, provide model scores.

* `B1`: number of repetitions for the paired Bootstrap used to create model score.

* `Bindex1`: n by B1 index matrix for paired bootstrap in the model score step. If Bindex1 == NULL, use the default Bootstrap, otherwise, use the provide matrix.

*`trace`: if trace == True, print the p-values as they are calculated.

#### Value
* `model0`: original model sequence

* `models`: list of proximal model sequences

* `errors0`: original error matrix

* `errors`: list of proximal model matrices

* `debiased_errors0`: de-biased estimate of the prediction error for the original process

* `debiased_errors`: de-biased estimates of the prediction error for the processes excluding a each of the selected feature

* `worsen`: estimated increase in prediction error

* `p_value`: p values for proximity analysis

* `selection_frequency`: frequency of features in S being selected

* `model_score`: model_score for proximity analysis

* `result_table`: organized result table

#### Examples
data(prostateCancerData)

data_train = prostateCancerData$train

data_test = prostateCancerData$test

x = data_train$feature

y = data_train$response

nams = data_train$names

n=length(y)

set.seed(483)

R1 = proximity.glmnet(X = x, Y = y, family = "gaussian", nfolds = 10,
                     nlambda = 30, standardize = F, alpha = .1,epsilon = 0.05^2, epsilon2 = .05^2,
                     B = 1000, B1 = 20)
                     
print(round(R1$result_table,3))

<a id="supervised1"></a>

## getIndex

The function getIndex chooses a model  based on randomized error curve. The users can use it to pick a model for any supervised learning algorithms they want to implement.

### Usage

getIndex(errors0, alpha = 0.1, epsilon = 0.1, selectionType = 0,
  one_sds = rep(0, ncol(errors0)))
  
### Arguments
* `errors0`: the n by m errors of the original model sequence

* `alpha`: added error with covariance structure*alpha, by default, alpha = .1

* `epsilon`: added error with covariance structure being identity times min(covariance diagonal) times epsilon, by default, epsilon = 0.1

* `selectionType`: if selectionType == 0, pick the model with the smallest randomized error

* `one_sds`: if the selectionType is 1, the we choose the model with smallest index such that model error(randomized) <= one_sds[i] + min error (randomized).

### Value
* `model_index`: selected model index in the original model sequences based on the randomized error curve.
  

<a id="supervised2"></a>


  
## nextdoor

We can also apply the Nextdoor analysis to other supervised algorithms using the function proximity when the original model is picked using the function getIndex. It requires the user to have the prediction errors for the original model sequence and the errors for the next-door model sequences available.

### Usage

nextdoor(errors0, errors, S, nams=NULL, K = 100, B = 1000, alpha = 0.1, epsilon = 0.05^2, epsilon2 = 0.05^2,Bindex = NULL,pv = TRUE,  rescale = TRUE, selectionType = 0, one_sds = rep(0, ncol(errors0)),trace = T)
### Arguments

* `errors0`: the n by m errors of the original model sequence

* `errors`: a list of n by m errors of the proximal model sequences

* `S`: a vector corresponding of names of the proximal sequences (name in errors)

* `nams`: a length p vector containing feature names. By default, nams = NULL, the features are named according to their column indexes in x.

* `K`: number of repetitions estimating the de-biased error

* `B`: number of bootstrap repetitions

* `alpha`: added error with covariance structure*alpha, by default, alpha = .1

* `epsilon`: added error with covariance structure being indentity times min(covariance diagonal) times epsilon, by default, epsilon =  0.05^2.

* `epsilon2`: added error in the Bootstrap step being min(covariance diagonal) times epsilon2, by default, epsilon2 = 0.05^2

* `Bindex`: n by B index matrix for bootstrap. if Bindex == NULL, use the default Bootstrap, otherwise, use the provide matrix.

* `pv`: if pv == True, estimate the p-values

* `rescale`: if rescale == True, perform the mean-rescaled Bootstrap

* `selectionType`: if selectionType == 0, pick the model with the smallest randomized error if selectionType == 1, use the 1se rule

* `one_sds`: if the selectionType is 1, the we choose the model with smallest index such that model error(randomized) <= one_sds[i] + min error(randomized)

* `trace`: if trace == True, print the p-value process

### Value
* `debiased_errors0`: de-biased estimate of the model error for the original procedure

* `debiased_errors`: de-biased estimate of the model error for the process excluding a specific feature

* `worsen`: estimated increase in prediction error

* `pv_value`: p values for proximity analysis

* `result_table`: organized result table
### Example
set.seed(48)
alphas = c(1:10)/10; 
errors0 = array(NA, dim = c(length(data$response),length(alphas))); errors=list()
for(i in 1:length(alphas)){
    model0 = ranger(response~., data = data, alpha = alphas[i])
    errors0[,i] = (data$response-model0$predictions)^2
    for(j in 1:length(nams)){
       if(is.null(errors[j][[1]])){errors[[j]] = array(0, dim = c(length(data$response),length(alphas)))}
        data1 = data[,-j];model0 = ranger(response~., data = data1, alpha = alphas[i]) 
        errors[[j]][,i] = (data$response-model0$predictions)^2
     }
 }
res = nextdoor(errors0 = errors0, errors = errors, S =c(1:length(nams)), nams=nams, B = 1000, alpha = 0.1, pv = TRUE,rescale = TRUE, selectionType = 0,trace = TRUE)

